# poemGPT
 A Decoder-only transformer trained on poems
- The Model is based on Andrej Kaparthy's Nano GPT
- The Model is a Decoder only Model which technically 'Blabbers" words based on the poems it has been trained on
- the image in the repository is the visualisation of the basic architecture 
- In the code u can find the use of dropout layers but while experimenting with different training parameters it was observed that not having a dropout layer was resulting in better performance
- The results are in constraint with available compute on my local machine 
- A detailed Commented out version with more experimentation will be uploaded soon
  
